{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFh5sh21y7zn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile('/content/BBC News Train.csv (1).zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('extracted_data')"
      ],
      "metadata": {
        "id": "bksdbcKMzNUf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"extracted_data/BBC News Train.csv\", 'r') as csvfile:\n",
        "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"The second line (first data point) looks like this:\\n\\n{csvfile.readline()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDUt91BCzngl",
        "outputId": "0b44d79c-bb18-46f9-8db5-30ef341773d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First line (header) looks like this:\n",
            "\n",
            "ArticleId,Text,Category\n",
            "\n",
            "The second line (first data point) looks like this:\n",
            "\n",
            "1833,worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.,business\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the global variables\n",
        "VOCAB_SIZE = 1000\n",
        "EMBEDDING_DIM = 16\n",
        "MAX_LENGTH = 120\n",
        "TRAINING_SPLIT = 0.8"
      ],
      "metadata": {
        "id": "BRxJQxouzrxz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/content/extracted_data/BBC News Train.csv\"\n",
        "data = np.loadtxt(data_dir, delimiter=',', skiprows=1, dtype='str', comments=None)\n",
        "print(f\"Shape of the data: {data.shape}\")\n",
        "print(f\"{data[0]}\\n{data[1]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y14RBS1yz11u",
        "outputId": "4543bb4a-be06-4eec-fed7-09671b5aff8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the data: (1490, 3)\n",
            "['1833'\n",
            " 'worldcom ex-boss launches defence lawyers defending former worldcom chief bernie ebbers against a battery of fraud charges have called a company whistleblower as their first witness.  cynthia cooper  worldcom s ex-head of internal accounting  alerted directors to irregular accounting practices at the us telecoms giant in 2002. her warnings led to the collapse of the firm following the discovery of an $11bn (£5.7bn) accounting fraud. mr ebbers has pleaded not guilty to charges of fraud and conspiracy.  prosecution lawyers have argued that mr ebbers orchestrated a series of accounting tricks at worldcom  ordering employees to hide expenses and inflate revenues to meet wall street earnings estimates. but ms cooper  who now runs her own consulting business  told a jury in new york on wednesday that external auditors arthur andersen had approved worldcom s accounting in early 2001 and 2002. she said andersen had given a  green light  to the procedures and practices used by worldcom. mr ebber s lawyers have said he was unaware of the fraud  arguing that auditors did not alert him to any problems.  ms cooper also said that during shareholder meetings mr ebbers often passed over technical questions to the company s finance chief  giving only  brief  answers himself. the prosecution s star witness  former worldcom financial chief scott sullivan  has said that mr ebbers ordered accounting adjustments at the firm  telling him to  hit our books . however  ms cooper said mr sullivan had not mentioned  anything uncomfortable  about worldcom s accounting during a 2001 audit committee meeting. mr ebbers could face a jail sentence of 85 years if convicted of all the charges he is facing. worldcom emerged from bankruptcy protection in 2004  and is now known as mci. last week  mci agreed to a buyout by verizon communications in a deal valued at $6.75bn.'\n",
            " 'business']\n",
            "['154'\n",
            " 'german business confidence slides german business confidence fell in february knocking hopes of a speedy recovery in europe s largest economy.  munich-based research institute ifo said that its confidence index fell to 95.5 in february from 97.5 in january  its first decline in three months. the study found that the outlook in both the manufacturing and retail sectors had worsened. observers had been hoping that a more confident business sector would signal that economic activity was picking up.   we re surprised that the ifo index has taken such a knock   said dz bank economist bernd weidensteiner.  the main reason is probably that the domestic economy is still weak  particularly in the retail trade.  economy and labour minister wolfgang clement called the dip in february s ifo confidence figure  a very mild decline . he said that despite the retreat  the index remained at a relatively high level and that he expected  a modest economic upswing  to continue.  germany s economy grew 1.6% last year after shrinking in 2003. however  the economy contracted by 0.2% during the last three months of 2004  mainly due to the reluctance of consumers to spend. latest indications are that growth is still proving elusive and ifo president hans-werner sinn said any improvement in german domestic demand was sluggish. exports had kept things going during the first half of 2004  but demand for exports was then hit as the value of the euro hit record levels making german products less competitive overseas. on top of that  the unemployment rate has been stuck at close to 10% and manufacturing firms  including daimlerchrysler  siemens and volkswagen  have been negotiating with unions over cost cutting measures. analysts said that the ifo figures and germany s continuing problems may delay an interest rate rise by the european central bank. eurozone interest rates are at 2%  but comments from senior officials have recently focused on the threat of inflation  prompting fears that interest rates may rise.'\n",
            " 'business']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function\n",
        "print(f\"There are {len(data)} sentence-label pairs in the dataset.\\n\")\n",
        "print(f\"First sentence has {len((data[0,1]).split())} words.\\n\")\n",
        "print(f\"The first 5 labels are {data[:5,2]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hx7swoRz5hJ",
        "outputId": "6ddc403d-a2d8-4632-ec07-4d2bff436ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1490 sentence-label pairs in the dataset.\n",
            "\n",
            "First sentence has 301 words.\n",
            "\n",
            "The first 5 labels are ['business' 'business' 'business' 'tech' 'business']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_val_datasets(data):\n",
        "    # Define the training size (e.g., 80% of the total data)\n",
        "    train_size = int(0.8 * len(data))\n",
        "\n",
        "    # Slice the dataset to get texts and labels\n",
        "    texts = data[:, 1]\n",
        "    labels = data[:, 2]\n",
        "\n",
        "    # Split the sentences and labels into train/validation sets\n",
        "    train_texts = texts[:train_size]\n",
        "    validation_texts = texts[train_size:]\n",
        "    train_labels = labels[:train_size]\n",
        "    validation_labels = labels[train_size:]\n",
        "\n",
        "    # Create the train and validation datasets from the splits\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((train_texts, train_labels))\n",
        "    validation_dataset = tf.data.Dataset.from_tensor_slices((validation_texts, validation_labels))\n",
        "\n",
        "\n",
        "    return train_dataset, validation_dataset\n",
        "\n",
        "# Create the datasets\n",
        "train_dataset, validation_dataset = train_val_datasets(data)\n",
        "print('Name: SNEHA HV      Register Number: 212222040157')\n",
        "print(f\"There are {train_dataset.cardinality()} sentence-label pairs for training.\\n\")\n",
        "print(f\"There are {validation_dataset.cardinality()} sentence-label pairs for validation.\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaGAGYNU0Aaw",
        "outputId": "475c214e-083b-4603-d3e9-d953c151eb06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: SNEHA HV      Register Number: 212222040157\n",
            "There are 1192 sentence-label pairs for training.\n",
            "\n",
            "There are 298 sentence-label pairs for validation.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_func(sentence):\n",
        "    # List of stopwords\n",
        "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"her\", \"here\",  \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\",  \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\",  \"should\", \"so\", \"some\", \"such\", \"than\", \"that\",  \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"these\", \"they\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\",  \"were\", \"what\",  \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"why\", \"with\", \"would\", \"you\",  \"your\", \"yours\", \"yourself\", \"yourselves\", \"'m\",  \"'d\", \"'ll\", \"'re\", \"'ve\", \"'s\", \"'d\"]\n",
        "\n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = tf.strings.lower(sentence)\n",
        "\n",
        "    # Remove stopwords\n",
        "    for word in stopwords:\n",
        "        if word[0] == \"'\":\n",
        "            sentence = tf.strings.regex_replace(sentence, rf\"{word}\\b\", \"\")\n",
        "        else:\n",
        "            sentence = tf.strings.regex_replace(sentence, rf\"\\b{word}\\b\", \"\")\n",
        "\n",
        "    # Remove punctuation\n",
        "    sentence = tf.strings.regex_replace(sentence, r'[!\"#$%&()\\*\\+,-\\./:;<=>?@\\[\\\\\\]^_`{|}~\\']', \"\")\n",
        "\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "7Lvg1kGJ0KnO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def fit_vectorizer(train_sentences, standardize_func):\n",
        "\n",
        "    # Instantiate the TextVectorization class, passing in the correct values for the parameters\n",
        "    vectorizer = tf.keras.layers.TextVectorization(\n",
        "        standardize=standardize_func,            # Custom standardization function\n",
        "        max_tokens=VOCAB_SIZE,                   # Maximum vocabulary size\n",
        "        output_sequence_length=MAX_LENGTH        # Truncate sequences to this length\n",
        "    )\n",
        "\n",
        "    # Adapt the vectorizer to the training sentences\n",
        "    vectorizer.adapt(train_sentences)\n",
        "\n",
        "\n",
        "    return vectorizer\n"
      ],
      "metadata": {
        "id": "AMbLZCmC0Npy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create the vectorizer\n",
        "text_only_dataset = train_dataset.map(lambda text, label: text)\n",
        "vectorizer = fit_vectorizer(text_only_dataset, standardize_func)\n",
        "vocab_size = vectorizer.vocabulary_size()\n",
        "print('Name: SNEHA HV     Register Number: 212222040157  ')\n",
        "print(f\"Vocabulary contains {vocab_size} words\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivZ1Aqqw0NmS",
        "outputId": "ff02fcd0-a55a-4d8b-9f63-8d1bbc236513"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: SNEHA HV     Register Number: 212222040157  \n",
            "Vocabulary contains 1000 words\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_label_encoder(train_labels, validation_labels):\n",
        "\n",
        "    # Concatenate the training and validation label datasets\n",
        "    labels = train_labels.concatenate(validation_labels)\n",
        "\n",
        "    # Instantiate the StringLookup layer without any OOV token\n",
        "    label_encoder = tf.keras.layers.StringLookup(num_oov_indices=0)\n",
        "\n",
        "    # Adapt the StringLookup layer on the combined labels dataset\n",
        "    label_encoder.adapt(labels)\n",
        "\n",
        "\n",
        "    return label_encoder\n"
      ],
      "metadata": {
        "id": "_vkcdfhB0Nas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the label encoder\n",
        "train_labels_only = train_dataset.map(lambda text, label: label)\n",
        "validation_labels_only = validation_dataset.map(lambda text, label: label)\n",
        "\n",
        "label_encoder = fit_label_encoder(train_labels_only,validation_labels_only)\n",
        "print('Name: SNEHA HV     Register Number: 212222040157    ')\n",
        "print(f'Unique labels: {label_encoder.get_vocabulary()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhUSRG3-0MrK",
        "outputId": "6a52a132-76d1-456c-d579-293203cb45e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: SNEHA HV     Register Number: 212222040157    \n",
            "Unique labels: ['sport', 'business', 'politics', 'entertainment', 'tech']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def preprocess_dataset(dataset, vectorizer, label_encoder, batch_size=32):\n",
        "    # Define a mapping function to preprocess each (text, label) pair\n",
        "    def preprocess(text, label):\n",
        "        text = vectorizer(text)                # Vectorize the text\n",
        "        label = label_encoder(label)           # Encode the label\n",
        "        return text, label\n",
        "\n",
        "    # Map the preprocessing function to the dataset and batch it\n",
        "    dataset = dataset.map(preprocess).batch(batch_size)\n",
        "\n",
        "    return dataset\n",
        "# Preprocess your dataset\n",
        "train_proc_dataset = preprocess_dataset(train_dataset, vectorizer, label_encoder)\n",
        "validation_proc_dataset = preprocess_dataset(validation_dataset, vectorizer, label_encoder)\n",
        "\n",
        "train_batch = next(train_proc_dataset.as_numpy_iterator())\n",
        "validation_batch = next(validation_proc_dataset.as_numpy_iterator())\n",
        "print('Name: SNEHA HV    Register Number: 212222040157     ')\n",
        "print(f\"Shape of the train batch: {train_batch[0].shape}\")\n",
        "print(f\"Shape of the validation batch: {validation_batch[0].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KuKtQit0g1r",
        "outputId": "99c00fc1-594f-44c9-bd02-95a7fe38daa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: SNEHA HV    Register Number: 212222040157     \n",
            "Shape of the train batch: (32, 120)\n",
            "Shape of the validation batch: (32, 120)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model():\n",
        "\n",
        "    # Define your model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(MAX_LENGTH,)),\n",
        "        tf.keras.layers.Embedding(input_dim=VOCAB_SIZE, output_dim=EMBEDDING_DIM),\n",
        "        tf.keras.layers.GlobalAveragePooling1D(),\n",
        "        tf.keras.layers.Dense(16, activation='relu'),\n",
        "        tf.keras.layers.Dense(5, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    # Compile model. Set an appropriate loss, optimizer and metrics\n",
        "    model.compile(\n",
        "        loss='sparse_categorical_crossentropy',  # or 'categorical_crossentropy' if labels are one-hot encoded\n",
        "        optimizer='adam',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "model = create_model()"
      ],
      "metadata": {
        "id": "OLX50Ggl0y3l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch = train_proc_dataset.take(1)\n",
        "\n",
        "try:\n",
        "\tmodel.evaluate(example_batch, verbose=False)\n",
        "except:\n",
        "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function and last layer are compatible with one another.\")\n",
        "else:\n",
        "\tpredictions = model.predict(example_batch, verbose=False)\n",
        "\tprint(f\"predictions have shape: {predictions.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qTEjF4Ti0yZc",
        "outputId": "9c2bc33b-c466-4ef5-a062-846f1a6c0559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "predictions have shape: (32, 5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_proc_dataset, epochs=30, validation_data=validation_proc_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIdc3zNm0yMI",
        "outputId": "a36055f9-50eb-4509-afd5-241edcca717b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.2336 - loss: 1.6037 - val_accuracy: 0.2047 - val_loss: 1.5842\n",
            "Epoch 2/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.2955 - loss: 1.5677 - val_accuracy: 0.2785 - val_loss: 1.5418\n",
            "Epoch 3/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 55ms/step - accuracy: 0.3527 - loss: 1.5086 - val_accuracy: 0.3557 - val_loss: 1.4636\n",
            "Epoch 4/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 123ms/step - accuracy: 0.4540 - loss: 1.4148 - val_accuracy: 0.4832 - val_loss: 1.3575\n",
            "Epoch 5/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.5511 - loss: 1.2905 - val_accuracy: 0.5503 - val_loss: 1.2290\n",
            "Epoch 6/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 63ms/step - accuracy: 0.6069 - loss: 1.1470 - val_accuracy: 0.6342 - val_loss: 1.0918\n",
            "Epoch 7/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - accuracy: 0.6594 - loss: 1.0037 - val_accuracy: 0.6879 - val_loss: 0.9653\n",
            "Epoch 8/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 101ms/step - accuracy: 0.7111 - loss: 0.8771 - val_accuracy: 0.7215 - val_loss: 0.8603\n",
            "Epoch 9/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 178ms/step - accuracy: 0.7462 - loss: 0.7728 - val_accuracy: 0.7483 - val_loss: 0.7762\n",
            "Epoch 10/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 56ms/step - accuracy: 0.7788 - loss: 0.6889 - val_accuracy: 0.7919 - val_loss: 0.7080\n",
            "Epoch 11/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.8115 - loss: 0.6203 - val_accuracy: 0.8121 - val_loss: 0.6512\n",
            "Epoch 12/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 90ms/step - accuracy: 0.8489 - loss: 0.5625 - val_accuracy: 0.8289 - val_loss: 0.6031\n",
            "Epoch 13/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - accuracy: 0.8682 - loss: 0.5121 - val_accuracy: 0.8490 - val_loss: 0.5611\n",
            "Epoch 14/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 57ms/step - accuracy: 0.8870 - loss: 0.4669 - val_accuracy: 0.8591 - val_loss: 0.5235\n",
            "Epoch 15/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 59ms/step - accuracy: 0.8953 - loss: 0.4257 - val_accuracy: 0.8624 - val_loss: 0.4896\n",
            "Epoch 16/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.9133 - loss: 0.3880 - val_accuracy: 0.8691 - val_loss: 0.4592\n",
            "Epoch 17/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 70ms/step - accuracy: 0.9235 - loss: 0.3537 - val_accuracy: 0.8758 - val_loss: 0.4316\n",
            "Epoch 18/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.9386 - loss: 0.3227 - val_accuracy: 0.8792 - val_loss: 0.4074\n",
            "Epoch 19/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 56ms/step - accuracy: 0.9448 - loss: 0.2952 - val_accuracy: 0.8826 - val_loss: 0.3863\n",
            "Epoch 20/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 88ms/step - accuracy: 0.9467 - loss: 0.2708 - val_accuracy: 0.8792 - val_loss: 0.3684\n",
            "Epoch 21/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 56ms/step - accuracy: 0.9490 - loss: 0.2492 - val_accuracy: 0.8826 - val_loss: 0.3527\n",
            "Epoch 22/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 58ms/step - accuracy: 0.9555 - loss: 0.2300 - val_accuracy: 0.8926 - val_loss: 0.3389\n",
            "Epoch 23/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.9574 - loss: 0.2129 - val_accuracy: 0.8960 - val_loss: 0.3268\n",
            "Epoch 24/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 71ms/step - accuracy: 0.9586 - loss: 0.1975 - val_accuracy: 0.8960 - val_loss: 0.3162\n",
            "Epoch 25/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 56ms/step - accuracy: 0.9629 - loss: 0.1837 - val_accuracy: 0.8993 - val_loss: 0.3066\n",
            "Epoch 26/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.9656 - loss: 0.1711 - val_accuracy: 0.9027 - val_loss: 0.2983\n",
            "Epoch 27/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.9687 - loss: 0.1597 - val_accuracy: 0.9094 - val_loss: 0.2908\n",
            "Epoch 28/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 58ms/step - accuracy: 0.9733 - loss: 0.1491 - val_accuracy: 0.9161 - val_loss: 0.2841\n",
            "Epoch 29/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 86ms/step - accuracy: 0.9738 - loss: 0.1395 - val_accuracy: 0.9161 - val_loss: 0.2780\n",
            "Epoch 30/30\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 58ms/step - accuracy: 0.9744 - loss: 0.1307 - val_accuracy: 0.9161 - val_loss: 0.2724\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_graphs(history, metric):\n",
        "    plt.plot(history.history[metric])\n",
        "    plt.plot(history.history[f'val_{metric}'])\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric)\n",
        "    plt.legend([metric, f'val_{metric}'])\n",
        "    plt.show()\n",
        "print('Name: SNEHA HV  Register Number: 212222040157 ')\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ],
      "metadata": {
        "id": "c9gD_cNY1FcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vmcCmk3t1Jwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New section"
      ],
      "metadata": {
        "id": "T9LJng6iy-9y"
      }
    }
  ]
}